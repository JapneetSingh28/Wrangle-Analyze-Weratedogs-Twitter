{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I'll be wrangling is the tweet archive of Twitter user @dog_rates\n",
    "(https://twitter.com/dog_rates), also known as WeRateDogs. This archive/dataset consists of 2356 basic\n",
    "tweet data from November, 2015 to August, 2017. WeRateDogs is a Twitter account that rates people's\n",
    "dogs with a humorous comment about the dog.\n",
    "Based on the images in the above dataset (i.e. WeRateDogs Twitter archive), another dataset is\n",
    "created which consists of image predictions (the top three only) alongside each tweet ID, image URL,\n",
    "and the image number that corresponded to the most confident prediction (numbered 1 to 4 since\n",
    "tweets can have up to four images). Though no wrangling will be done directly on this image predictions\n",
    "dataset, it will definitely provide some additional data for our main tweet archive dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather Twitter archive CSV file\n",
    "\n",
    "Using the link provided by Udacity, I downloaded the WeRateDogs Twitter archive manually as\n",
    "twitter_archive_enhanced.csv\n",
    "(https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archiveenhanced/twitter-archive-enhanced.csv) file and imported this file into a dataframe (df_twitter_archive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather tweet image predictions\n",
    "\n",
    "I downloaded the tweet image predictions file hosted on Udacity's servers programmatically using\n",
    "Python's Requests library and saved it locally to image_predictions.tsv file. Then, I imported this file\n",
    "into a Python Pandas dataframe (df_image_predictions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather data from Twitter API\n",
    "\n",
    "Using the tweet IDs in the Twitter archive, I accessed the entire data for every tweet from Twitter API\n",
    "and stored every tweet's entire set of JSON data in a file called tweet_json.txt file. Created a\n",
    "dataframe status_df from this JSON including only tweet_id, retweet_count, favorite_count and\n",
    "display_text_range data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I was able to identify 2 quality issues just by going through the Key Points in the Project\n",
    "Motivation page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I opened the twitter_archive_enhanced.csv and image_predictions.tsv in Excel and scrolled\n",
    "through them, looking for quality and tidiness issues. I was able to spot the followng 2 quality and 2\n",
    "tidiness issues:\n",
    "Quality: unnecessary html tags in source column of twitter archive in place of utility name e.g.\n",
    "<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for\n",
    "iPhone</a>\n",
    "Quality: text column of twitter archive contains untruncated text instead of displayable text\n",
    "Tidiness: doggo, floofer, pupper and puppo columns in arc_df table should be merged into\n",
    "one column named \"stage\"\n",
    "Tidiness: Twitter archive data without any duplicates (i.e. retweets) will have empty\n",
    "retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp columns,\n",
    "which can be dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Programmatic Assessment\n",
    "I used pandas' info method on df_twitter_archive to spot erroneous datatypes and other quality issues, if any.\n",
    "Then I used value_counts method on rating_numerator, rating_denominator and name columns to\n",
    "look up the range of their values and its distribution. Also to verify 1 tidiness issue that I found during\n",
    "the visual assessment, I queried the archive dataframe to see if any of its tweets has more than one\n",
    "dog-stage mentioned. This entire activity helped me to identify the following quality issues.\n",
    "Following are the observations summary from the above performed Assessments-\n",
    "missing some expanded_urls,\n",
    "rating_numerator column has values less than 10 as well as some very large numbers (e.g. 1176),\n",
    "rating_denominator column has values other than 10,\n",
    "some entries should be classified as puppers (missing data),\n",
    "343rd entry is not a dog rating,\n",
    "name is sometimes not an actual name,\n",
    "wrong data types (in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, timestamp and retweeted_status_timestamp),\n",
    "some records have more than one dog stage,\n",
    "tables also have mising datas,\n",
    "incorrect rating_numerator and rating_denominator for 1069th, 1166th, 2336th entries,\n",
    "p1, p2, p3 inconsistent capitalization (sometimes first letter is capital),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.Cleaning Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning though was definitely the most time consuming part of wrangling the data. It was so surprising how much could had to be adjusted and how some of the issues really had to be looked at closely. Even considering that, I didn't clean everything! It was stated in the project's requirements that we didn't need to clean all issues, but I could see that cleaning can easily take a large amount of time before even doing any analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data\n",
    "After the completion of the cleaning process, I stored the df_clean_final DataFrame in\n",
    "twitter_archive_master.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
